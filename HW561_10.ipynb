{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 10\n",
    "### Jingrong Tian\n",
    "The following three cells are methods for backtracking, log-barrier and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack(x0, dx, f, df0, alpha, beta, verbose=False):\n",
    "    '''\n",
    "    Backtracking for general functions\n",
    "    :param x0: initial point\n",
    "    :param dx: incremental factor for updating x0\n",
    "    :param f: objective function\n",
    "    :param df0: gradient of f at x0\n",
    "    :param alpha: sloping factor of stopping criterion\n",
    "    :param beta: aggressiveness parameter for backtracking steps\n",
    "    :param verbose: if True, return the objective function \n",
    "        value at the final x\n",
    "    '''\n",
    "    \n",
    "    # np.dot corresponds to a tensor product\n",
    "    delta = alpha * np.dot(dx, df0)\n",
    "    \n",
    "    t = 1\n",
    "    f0 = f(x0)\n",
    "    x = x0 + dx  # t = 1\n",
    "    fx = f(x)\n",
    "    \n",
    "    while (not np.isfinite(fx)) or fx > f0 + delta * t:\n",
    "        # this is cheaper than exponentiating beta each time\n",
    "        t = beta * t\n",
    "        x = x0 + t * dx\n",
    "        fx = f(x)\n",
    "        \n",
    "    if verbose:\n",
    "        return x, f(x)\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "\n",
    "def backtrack_n(n, x0, f, grad_f, alpha, beta, hess_f=None):\n",
    "    '''\n",
    "    Perform n steps of backtracking using the specified increment type\n",
    "    :param n: number of backtracking steps\n",
    "    :param x0: initial point\n",
    "    :param f: objective function\n",
    "    :param grad_f: gradient of objective function\n",
    "    :param alpha: sloping factor of stopping criterion\n",
    "    :param beta: aggressiveness parameter for backtracking steps\n",
    "    :param hess_f: optional Hessian of objective function, \n",
    "        invokes Newton's method\n",
    "    '''\n",
    "    x = x0\n",
    "    xs = x\n",
    "    fs = [f(x)]\n",
    "    \n",
    "    for i in range(n):\n",
    "        if hess_f is None:\n",
    "            dx = -grad_f(x)\n",
    "        else:\n",
    "            dx = -np.linalg.solve(hess_f(x), grad_f(x))\n",
    "        \n",
    "        x, fx = backtrack(\n",
    "            x0=x,\n",
    "            dx=dx,\n",
    "            f=f,\n",
    "            df0=grad_f(x),\n",
    "            alpha=alpha,\n",
    "            beta=beta,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        xs = np.append(xs, x)\n",
    "        fs.append(fx)\n",
    "        \n",
    "    # return a single numpy array with columns x_{i} and f(x)\n",
    "    return np.column_stack((xs.reshape((n + 1, x0.shape[0])), fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lb(f, lb_cons, grad_f, grad_lb_cons, x0, alpha, beta, M, \n",
    "       iter_init, iter_outer, iter_inner, \n",
    "       x_threshold=None, hess_f=None, hess_lb_cons=None,\n",
    "       **kwargs):\n",
    "    '''\n",
    "    Log-barrier method with Newton's method\n",
    "    :param f: objective function\n",
    "    :param lb_cons: log-barrier constraints\n",
    "    :param grad_f: gradient of objective function\n",
    "    :param grad_lb_cons: gradient of log-barrier constraints\n",
    "    :param x0: initial point\n",
    "    :param alpha: sloping factor of stopping criterion\n",
    "    :param beta: aggressiveness parameter for backtracking steps\n",
    "    :param M: increase factor for t\n",
    "    :param iter_init: number of centering iterations\n",
    "    :param iter_outer: number of outer iterations\n",
    "    :param iter_inner: number of inner iterations\n",
    "    :param x_threshold: optional vector the same length as x0; \n",
    "        the algorithm terminates if any component of the iterate\n",
    "        is less than the corresponding component of x_threshold\n",
    "    :param hess_f: optional Hessian of objective function, \n",
    "        required for Newton's method\n",
    "    :param hess_lb_cons: optional Hessian of log-barrier constraints,\n",
    "        required for Newton's method\n",
    "    :param kwargs: additional arguments passed to lb_cons,\n",
    "        grad_lb_cons, and hess_lb_cons\n",
    "    '''\n",
    "    x = x0\n",
    "    \n",
    "    # form the gradient and Hessian of the log-barrier function\n",
    "    def grad_flb(z, t=1):\n",
    "        return grad_f(z) - (1 / t) * grad_lb_cons(z, **kwargs)\n",
    "    \n",
    "    # only form the Hessian of the log-barrier function \n",
    "    # if the appropriate arguments were provided\n",
    "    if hess_f is None or hess_lb_cons is None:\n",
    "        hess_flb = None\n",
    "    else:\n",
    "        def hess_flb(z, t=1):\n",
    "            return hess_f(z) - (1 / t) * hess_lb_cons(z, **kwargs)\n",
    "    \n",
    "    \n",
    "    # centering steps\n",
    "    x = backtrack_n(\n",
    "        n=iter_init,\n",
    "        x0=x,\n",
    "        # form the log-barrier function on the \n",
    "        # fly to accommodate varying t\n",
    "        f=lambda z: f(z) - lb_cons(z, **kwargs),\n",
    "        grad_f=grad_flb,\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        hess_f=hess_flb\n",
    "    )[-1,:x0.shape[0]]\n",
    "    print('Centering complete: ', x)\n",
    "    \n",
    "    # begin log-barrier iterations\n",
    "    t = 1\n",
    "\n",
    "    if x_threshold is not None:\n",
    "        print('Threshold provided, algorithm may terminate early')\n",
    "    \n",
    "    i = 1\n",
    "    while i <= iter_outer:\n",
    "        if x_threshold is not None:\n",
    "            if any(x < x_threshold): break\n",
    "        \n",
    "        print('Outer iteration', i)\n",
    "        t = t * M        \n",
    "        \n",
    "        # we need something slightly different from backtrack_n\n",
    "        for j in range(iter_inner):            \n",
    "            if hess_flb is None:\n",
    "                dx = -grad_flb(x)\n",
    "            else:\n",
    "                dx = -np.linalg.solve(hess_flb(x, t), grad_flb(x, t))\n",
    "            \n",
    "            x = backtrack(\n",
    "                x0=x,\n",
    "                dx=dx,\n",
    "                # form the log-barrier function \n",
    "                f=lambda z: f(z) - (1 / t) * lb_cons(z, **kwargs),\n",
    "                df0=grad_flb(x, t),\n",
    "                alpha=alpha,\n",
    "                beta=beta\n",
    "            )\n",
    "            print('  Inner iteration', j, ': ', x)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up Two Sets of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_circle(N):\n",
    "    x = rd.randn(N, 2)\n",
    "    # normalize each row of x\n",
    "    return x / np.linalg.norm(x=x, ord=2, axis=1).reshape(N, 1)\n",
    "\n",
    "\n",
    "def random_radius(N, R=1):\n",
    "    r = rd.rand(N)\n",
    "    # ensure uniform sampling from the disc\n",
    "    return R * np.sqrt(r) \n",
    "    \n",
    "\n",
    "def random_disc(N, mu=[0,0], R=1):\n",
    "    x = random_circle(N)\n",
    "    r = random_radius(N, R=R)\n",
    "    return r.reshape(N, 1) * x + mu\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "# set seed for reproducibility\n",
    "rd.seed(0)\n",
    "X = np.concatenate(\n",
    "    (\n",
    "        random_disc(N, mu=[-2, -2]),\n",
    "        random_disc(N, mu=[2, 2])        \n",
    "    )\n",
    ")\n",
    "\n",
    "y = np.repeat([-1, 1], repeats=[10, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $\\mathbf{v}^{\\left(0\\right)}$, we can apply the method from Lecture 03 Part VIII to choose $z^{\\left(0\\right)}$, i.e., \n",
    "\n",
    "$$\n",
    "z^{\\left(0\\right)}=\\max\\left(h_{1}\\left(\\mathbf{v}^{\\left(0\\right)},b^{\\left(0\\right)}\\right),\\ldots,h_{20}\\left(\\mathbf{v}^{\\left(0\\right)},b^{\\left(0\\right)}\\right)\\right)+1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.01533498241895"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up v0\n",
    "v0 = np.array([-20, 20])\n",
    "b0 = 10\n",
    "\n",
    "\n",
    "def h(v, b, X, y):\n",
    "    '''\n",
    "    Constraint function\n",
    "    :param v: vector of length 2\n",
    "    :param b: scalar\n",
    "    :param X: matrix whose ith row is \\mathbf{x}^{i}\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}\n",
    "    '''\n",
    "    # compute \\mathbf{v}^{\\mathsf{T}}\\mathbf{x}^{i} for each i\n",
    "    prod = np.apply_along_axis(func1d=np.dot, axis=1, arr=X, b=v)\n",
    "    return 1 - y * (prod - b)\n",
    "\n",
    "z0 = max(h(v=v0, b=b0, X=X, y=y)) + 1\n",
    "z0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above result and radient descent for the inner backtracking steps of log-barrier function.\n",
    "\n",
    "$$\n",
    "\\tilde{f}_{\\text{lb}}\\left(\\mathbf{v},b,z\\right)=\\tilde{f}\\left(\\mathbf{v},b,z\\right)-\\sum_{i=0}^{19}\\frac{1}{t}\\log\\left(-\\tilde{h}_{i}\\left(\\mathbf{v},b,z\\right)\\right)=z-\\frac{1}{t}\\sum_{i=0}^{19}\\log\\left(z-h_{i}\\left(\\mathbf{v},b\\right)\\right).\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla\\tilde{f}_{\\text{lb}}\\left(\\mathbf{p}\\right)=\\begin{bmatrix}0\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "1\n",
    "\\end{bmatrix}-\\frac{1}{t}\\sum_{i=0}^{19}\\frac{1}{p_{4}-h_{i}\\left(p_{1},p_{2},p_{3}\\right)}\\begin{bmatrix}x_{1}^{\\left(i\\right)}y^{\\left(i\\right)}\\\\\n",
    "x_{2}^{\\left(i\\right)}y^{\\left(i\\right)}\\\\\n",
    "-y^{\\left(i\\right)}\\\\\n",
    "1\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the objective functions and constraints\n",
    "f_tilde = lambda p: p[3]\n",
    "grad_f_tilde = lambda p: np.array([0, 0, 0, 1])\n",
    "\n",
    "\n",
    "def lb_cons_tilde(p, h, X, y):\n",
    "    '''\n",
    "    Log-barrier constraints\n",
    "    :param p: point at which to evaluate constraints\n",
    "    :param h: constraint function\n",
    "    :param X: matrix whose ith row is \\mathbf{x}^{i}\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}\n",
    "    '''\n",
    "    return np.sum(np.log(p[3] - h(v=p[:2], b=p[2], X=X, y=y)))\n",
    "\n",
    "\n",
    "def grad_lb_cons_tilde(p, h, X, y):\n",
    "    '''\n",
    "    Gradient of log-barrier constraints\n",
    "    :param p: point at which to evaluate gradient\n",
    "    :param h: constraint function\n",
    "    :param X: matrix whose ith row is \\mathbf{x}^{i}\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}\n",
    "    '''\n",
    "    # build a matrix whose ith row is \n",
    "    # [x_{1}^{i} * y^{i}, x_{2}^{i} * y^{i}, -y^{i}, 1]\n",
    "    M = np.concatenate(\n",
    "        (\n",
    "            X * y[:, np.newaxis], \n",
    "            -y[:, np.newaxis],\n",
    "            np.ones(y.shape[0])[:, np.newaxis]\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    c = 1 / (p[3] - h(v=p[:2], b=p[2], X=X, y=y))\n",
    "\n",
    "    # column sums of the Hadamard product of M and c\n",
    "    return np.sum(M * c[:, np.newaxis], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centering complete:  [-7.79589092 32.36949547  7.71317363 12.72537674]\n",
      "Threshold provided, algorithm may terminate early\n",
      "Outer iteration 1\n",
      "  Inner iteration 0 :  [-7.18380815 33.01408622  7.69064768 12.07337406]\n",
      "  Inner iteration 1 :  [-6.58932751 33.64055225  7.67017364 11.41165308]\n",
      "  Inner iteration 2 :  [-6.01068492 34.25068655  7.65149076 10.7411792 ]\n",
      "  Inner iteration 3 :  [-5.44639269 34.84600343  7.63438563 10.06276765]\n",
      "  Inner iteration 4 :  [-4.89518202 35.42779609  7.61868137  9.37711459]\n",
      "Outer iteration 2\n",
      "  Inner iteration 0 :  [-4.35596016 35.9971797   7.60422965  8.68482025]\n",
      "  Inner iteration 1 :  [-3.82777781 36.5551242   7.59090488  7.98640665]\n",
      "  Inner iteration 2 :  [-3.30980388 37.10247966  7.57859975  7.28233123]\n",
      "  Inner iteration 3 :  [-2.80130579 37.63999617  7.56722184  6.57299756]\n",
      "  Inner iteration 4 :  [-2.30163378 38.16833969  7.55669099  5.85876393]\n",
      "Outer iteration 3\n",
      "  Inner iteration 0 :  [-1.81020828 38.68810478  7.5469372   5.13995017]\n",
      "  Inner iteration 1 :  [-1.32650968 39.19982493  7.53789898  4.41684322]\n",
      "  Inner iteration 2 :  [-0.85007002 39.70398105  7.52952204  3.68970171]\n",
      "  Inner iteration 3 :  [-0.38046595 40.2010085   7.52175819  2.95875974]\n",
      "  Inner iteration 4 :  [ 0.08268694 40.69130293  7.51456448  2.22423005]\n",
      "Outer iteration 4\n",
      "  Inner iteration 0 :  [ 0.53973902 41.17522522  7.50790242  1.48630665]\n",
      "  Inner iteration 1 :  [ 0.99101072 41.65310558  7.50173744  0.74516704]\n",
      "  Inner iteration 2 :  [1.43679596e+00 4.21252472e+01 7.49603834e+00 9.74165125e-04]\n",
      "  Inner iteration 3 :  [ 1.87736517 42.59192905  7.49077687 -0.74612198]\n",
      "  Inner iteration 4 :  [ 2.31296782 43.05340884  7.48592737 -1.49598297]\n"
     ]
    }
   ],
   "source": [
    "#calculation, calling functions\n",
    "p0 = lb(\n",
    "    f=f_tilde, \n",
    "    lb_cons=lb_cons_tilde, \n",
    "    grad_f=grad_f_tilde, \n",
    "    grad_lb_cons=grad_lb_cons_tilde, \n",
    "    x0=np.append(v0, [b0, z0]),\n",
    "    alpha=0.2, \n",
    "    beta=0.8, \n",
    "    M=10, \n",
    "    iter_init=10,\n",
    "    iter_outer=10, \n",
    "    iter_inner=5,\n",
    "    x_threshold=np.array([-np.inf, -np.inf, -np.inf, 0]),\n",
    "    h=h, X=X, y=y\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The result for part (a) is the last inner iteration of outer iteration 4, which is  [ 2.31296782 43.05340884  7.48592737 -1.49598297]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve the 3D program using the log-barrier method. Let\n",
    "\n",
    "$$\n",
    "f\\left(\\mathbf{v},b\\right)=\\frac{1}{2}\\left\\Vert \\mathbf{v}\\right\\Vert ^{2}=\\frac{1}{2}\\mathbf{v}^{\\mathsf{T}}\\mathbf{v},\n",
    "$$\n",
    "\n",
    "and let $\\mathbf{q}=\\left(\\mathbf{v},b\\right)$, so that the log-barrier function is\n",
    "\n",
    "$$\n",
    "f_{\\text{lb}}\\left(\\mathbf{q}\\right)=f\\left(\\mathbf{q}\\right)-\\frac{1}{t}\\sum_{i=0}^{19}\\log\\left(-h_{i}\\left(\\mathbf{q}\\right)\\right).\n",
    "$$\n",
    "\n",
    "We have shown in the accompanying PDF that\n",
    "\n",
    "$$\n",
    "\\nabla f_{\\text{lb}}\\left(\\mathbf{q}\\right)=\\begin{bmatrix}q_{1}\\\\\n",
    "q_{2}\\\\\n",
    "0\n",
    "\\end{bmatrix}-\\frac{1}{t}\\sum_{i=0}^{19}-\\frac{y^{\\left(i\\right)}}{h_{i}\\left(\\mathbf{q}\\right)}\\tilde{\\mathbf{x}}^{\\left(i\\right)}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\nabla^{2}f_{\\text{lb}}\\left(\\mathbf{q}\\right)=\\begin{bmatrix}1 & 0 & 0\\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}-\\frac{1}{t}\\sum_{i=0}^{19}-\\frac{1}{\\left(h_{i}\\left(\\mathbf{q}\\right)\\right)^{2}}\\tilde{\\mathbf{x}}^{\\left(i\\right)}\\left(\\tilde{\\mathbf{x}}^{\\left(i\\right)}\\right)^{\\mathsf{T}},\n",
    "$$\n",
    "\n",
    "where $\\tilde{\\mathbf{x}}^{\\left(i\\right)}=\\left(x_{1}^{\\left(i\\right)},x_{2}^{\\left(i\\right)},-1\\right)$. We now define the objective and constraint functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resetting the gradient, objective, and hession as below, then calling the py program 1b to calculate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda q: np.dot(q, q) / 2\n",
    "\n",
    "grad_f = lambda q: np.append(q[:2], 0)\n",
    "\n",
    "hess_f = lambda q: np.array([[1, 0, 0], [0, 1, 0], [0, 0, 0]])\n",
    "\n",
    "\n",
    "# build a matrix whose ith row is [x_{1}^{i}, x_{2}^{i}, -1]\n",
    "X_tilde = np.concatenate(\n",
    "    (\n",
    "        X, \n",
    "        -np.ones(X.shape[0])[:, np.newaxis]\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "def lb_cons(q, h, X_tilde, y):\n",
    "    '''\n",
    "    Log-barrier constraints\n",
    "    :param q: point at which to evaluate constraints\n",
    "    :param h: constraint function\n",
    "    :param X_tilde: matrix whose ith row is (\\mathbf{x}^{i}, -1)\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}\n",
    "    '''\n",
    "    return np.sum(np.log(-h(v=q[:2], b=q[2], X=X_tilde[:,:2], y=y)))\n",
    "\n",
    "\n",
    "def grad_lb_cons(q, h, X_tilde, y):\n",
    "    '''\n",
    "    Gradient of log-barrier constraints\n",
    "    :param q: point at which to evaluate gradient\n",
    "    :param h: constraint function\n",
    "    :param X_tilde: matrix whose ith row is (\\mathbf{x}^{i}, -1)\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}\n",
    "    '''\n",
    "    c = -y / h(v=q[:2], b=q[2], X=X_tilde[:,:2], y=y)\n",
    "\n",
    "    # column sums of the Hadamard product of M and c\n",
    "    return np.sum(X_tilde * c[:, np.newaxis], axis=0)\n",
    "\n",
    "\n",
    "def hess_lb_cons(q, h, X_tilde, y):\n",
    "    '''\n",
    "    Hessian of log-barrier constraints\n",
    "    :param q: point at which to evaluate Hessian\n",
    "    :param h: constraint function\n",
    "    :param X_tilde: matrix whose ith row is (\\mathbf{x}^{i}, -1)\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}\n",
    "    '''\n",
    "    # list whose ith element is \\tilde{x}^{i}(\\tilde{x}^{i})^{T}\n",
    "    xxT = [x[:, np.newaxis] * x for x in X_tilde]\n",
    "    \n",
    "    c = -1 / h(v=q[:2], b=q[2], X=X_tilde[:,:2], y=y) ** 2\n",
    "\n",
    "    # then sum the elements of the resulting list (element-wise)\n",
    "    return np.sum(\n",
    "        list(\n",
    "            # multiply each element of xxT (a matrix) by the \n",
    "            # corresponding element of c (a scalar)\n",
    "            map(lambda r, s: r * s, xxT, c)\n",
    "        ), \n",
    "        axis=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centering complete:  [2.35124983 2.57425822 2.07190756]\n",
      "Outer iteration 1\n",
      "  Inner iteration 0 :  [1.54742087 1.69250169 0.03385209]\n",
      "  Inner iteration 1 :  [1.00993524 1.09814664 0.45692042]\n",
      "Outer iteration 2\n",
      "  Inner iteration 0 :  [ 0.62950955  0.68395171 -0.01256503]\n",
      "  Inner iteration 1 :  [0.52137561 0.56363594 0.10962022]\n",
      "Outer iteration 3\n",
      "  Inner iteration 0 :  [0.41726686 0.45047657 0.05183293]\n",
      "  Inner iteration 1 :  [0.36561291 0.39164456 0.01799623]\n",
      "Outer iteration 4\n",
      "  Inner iteration 0 :  [0.36063277 0.38525316 0.00606433]\n",
      "  Inner iteration 1 :  [0.35895474 0.379472   0.00249235]\n",
      "Outer iteration 5\n",
      "  Inner iteration 0 :  [0.35892204 0.37835206 0.00325781]\n",
      "  Inner iteration 1 :  [0.3606204  0.37551381 0.00259428]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianjingrong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in log\n"
     ]
    }
   ],
   "source": [
    "q_ast = lb(\n",
    "    f=f, \n",
    "    lb_cons=lb_cons, \n",
    "    grad_f=grad_f, \n",
    "    grad_lb_cons=grad_lb_cons, \n",
    "    x0=p0[:3],\n",
    "    alpha=0.1, \n",
    "    beta=0.5, \n",
    "    M=10, \n",
    "    iter_init=3,\n",
    "    iter_outer=5, \n",
    "    iter_inner=2,\n",
    "    hess_f=hess_f,\n",
    "    hess_lb_cons=hess_lb_cons,\n",
    "    h=h, X_tilde=X_tilde, y=y\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The sol for part(b) is : Outer iteration 5 Inner iteration 1 :  [0.3606204  0.37551381 0.00259428]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement the algorithm described in the accompanying PDF. Step 1 requires that we compute\n",
    "\n",
    "$$\n",
    "\\hat{\\eta}\\left(\\mathbf{q},\\boldsymbol{\\lambda}\\right)=-\\left(\\mathbf{h}\\left(\\mathbf{q}\\right)\\right)^{\\mathsf{T}}\\boldsymbol{\\lambda}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta_hat(q, h, X, y, lmbda):\n",
    "    '''\n",
    "    Surrogate duality gap\n",
    "    :param q: point at which to evaluate gap\n",
    "    :param h: constraint function\n",
    "    :param X: matrix whose ith row is \\mathbf{x}^{i}\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}\n",
    "    :param lmbda: dual variable, vector whose length is \n",
    "        equal to the number of constraints\n",
    "    '''\n",
    "    return -np.dot(h(v=q[:2], b=q[2], X=X, y=y), lmbda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 requires that we compute the primal-dual search direction\n",
    "\n",
    "$$\n",
    "\\Delta\\mathbf{z}_{\\text{pd}}=\\begin{bmatrix}\\Delta\\mathbf{q}_{\\text{pd}}\\\\\n",
    "\\Delta\\boldsymbol{\\lambda}_{\\text{pd}}\n",
    "\\end{bmatrix}=-\\begin{bmatrix}\\nabla^{2}f\\left(\\mathbf{q}\\right) & \\left(D\\mathbf{h}\\left(\\mathbf{q}\\right)\\right)^{\\mathsf{T}}\\\\\n",
    "-\\text{diag}\\left(\\boldsymbol{\\lambda}\\right)D\\mathbf{h}\\left(\\mathbf{q}\\right) & -\\text{diag}\\left(\\mathbf{h}\\left(\\mathbf{q}\\right)\\right)\n",
    "\\end{bmatrix}^{-1}\n",
    "\\begin{bmatrix}\n",
    "r_{\\text{dual}} \\\\\n",
    "r_{\\text{cent}}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "D\\mathbf{h}\\left(\\mathbf{q}\\right)=\\begin{bmatrix}\\left(\\nabla h_{0}\\left(\\mathbf{q}\\right)\\right)^{\\mathsf{T}}\\\\\n",
    "\\left(\\nabla h_{1}\\left(\\mathbf{q}\\right)\\right)^{\\mathsf{T}}\\\\\n",
    "\\vdots\\\\\n",
    "\\left(\\nabla h_{19}\\left(\\mathbf{q}\\right)\\right)^{\\mathsf{T}}\n",
    "\\end{bmatrix}=-\\begin{bmatrix}y^{\\left(0\\right)}\\left(\\tilde{\\mathbf{x}}^{\\left(0\\right)}\\right)^{\\mathsf{T}}\\\\\n",
    "y^{\\left(1\\right)}\\left(\\tilde{\\mathbf{x}}^{\\left(1\\right)}\\right)^{\\mathsf{T}}\\\\\n",
    "\\vdots\\\\\n",
    "y^{\\left(19\\right)}\\left(\\tilde{\\mathbf{x}}^{\\left(19\\right)}\\right)^{\\mathsf{T}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "r_{\\text{dual}} \\\\\n",
    "r_{\\text{cent}}\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\\nabla f\\left(\\mathbf{q}\\right)+\\left(D\\mathbf{h}\\left(\\mathbf{q}\\right)\\right)^{\\mathsf{T}}\\boldsymbol{\\lambda}\\\\\n",
    "-\\text{diag}\\left(\\boldsymbol{\\lambda}\\right)\\mathbf{h}\\left(\\mathbf{q}\\right)-\\left(1/t\\right)\\mathbf{1}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "##From Prof Nate Strawn\n",
    "#################\n",
    "def Dh(X_tilde, y):\n",
    "    '''\n",
    "    Jacobian of constraint vector\n",
    "    :param X_tilde: matrix whose ith row is (\\mathbf{x}^{i}, -1)\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}    \n",
    "    '''\n",
    "    return -X_tilde * y[:, np.newaxis]\n",
    "\n",
    "\n",
    "def r_dual(q, grad_f, Dh, lmbda, **kwargs):\n",
    "    '''\n",
    "    Dual residual\n",
    "    :param q: current iterate\n",
    "    :param grad_f: gradient of objective function\n",
    "    :param Dh: Jacobian of constraint vector\n",
    "    :param lmbda: dual variable\n",
    "    :param kwargs: additional arguments passed to Dh\n",
    "    '''\n",
    "    return grad_f(q) + np.matmul(Dh(**kwargs).T, lmbda)\n",
    " \n",
    "    \n",
    "def r_cent(q, h, t, lmbda, X, y):\n",
    "    '''\n",
    "    Centrality residual\n",
    "    :param q: current iterate\n",
    "    :param h: constraint function\n",
    "    :param t: parameter associated with current surrogate duality gap\n",
    "    :param lmbda: dual variable\n",
    "    :param X: matrix whose ith row is \\mathbf{x}^{i}\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}\n",
    "    '''\n",
    "    return -np.matmul(\n",
    "        np.diag(lmbda), \n",
    "        h(v=q[:2], b=q[2], X=X, y=y)\n",
    "    ) - 1 / t\n",
    "\n",
    "\n",
    "def zpd(Dh, grad_f, h, hess_f, lmbda, q, \n",
    "        r_cent, r_dual, t, X_tilde, y):\n",
    "    '''\n",
    "    Compute primal-dual search direction\n",
    "    :param Dh: Jacobian of constraint vector\n",
    "    :param grad_f: gradient of objective function    \n",
    "    :param h: constraint function\n",
    "    :param hess_f: Hessian of objective function\n",
    "    :param lmbda: dual variable\n",
    "    :param q: current iterate\n",
    "    :param r_cent: centrality residual\n",
    "    :param r_dual: dual residual\n",
    "    :param t: parameter associated with current surrogate duality gap\n",
    "    :param X_tilde: matrix whose ith row is (\\mathbf{x}^{i}, -1)\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}        \n",
    "    '''\n",
    "    X = X_tilde[:,:2]\n",
    "    \n",
    "    # construct the matrix to be inverted\n",
    "    M = np.concatenate(\n",
    "        (\n",
    "            # first row\n",
    "            np.concatenate(\n",
    "                (\n",
    "                    hess_f(q=q), \n",
    "                    Dh(X_tilde=X_tilde, y=y).T\n",
    "                ),\n",
    "                axis=1\n",
    "            ),\n",
    "            # second row\n",
    "            np.concatenate(\n",
    "                (\n",
    "                    -np.matmul(\n",
    "                        np.diag(lmbda), Dh(X_tilde=X_tilde, y=y)\n",
    "                    ),\n",
    "                    -np.diag(h(v=q[:2], b=q[2], X=X, y=y))\n",
    "                ),\n",
    "                axis=1\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # construct r_t\n",
    "    rt = np.concatenate(\n",
    "        (\n",
    "            r_dual(\n",
    "                q=q, grad_f=grad_f, Dh=Dh, lmbda=lmbda, \n",
    "                X_tilde=X_tilde, y=y\n",
    "            ),\n",
    "            r_cent(q=q, h=h, t=t, lmbda=lmbda, X=X, y=y)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return -np.linalg.solve(M, rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd(alpha, beta, Dh, eta_hat, grad_f, h, hess_f, iter, \n",
    "       lmbda0, nu, q0, r_cent, r_dual, X_tilde, y):\n",
    "    '''\n",
    "    Primal-dual interior point method\n",
    "    :param alpha: sloping factor of stopping criterion\n",
    "    :param beta: aggressiveness parameter for backtracking steps    \n",
    "    :param Dh: Jacobian of constraint vector\n",
    "    :param eta_hat: surrogate duality gap\n",
    "    :param grad_f: gradient of objective function    \n",
    "    :param h: constraint function\n",
    "    :param hess_f: Hessian of objective function\n",
    "    :param iter: number of iterations to perform\n",
    "    :param lmbda0: initial dual variable\n",
    "    :param nu: parameter controlling t\n",
    "    :param q0: initial iterate\n",
    "    :param r_cent: centrality residual\n",
    "    :param r_dual: dual residual\n",
    "    :param X_tilde: matrix whose ith row is (\\mathbf{x}^{i}, -1)\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}    \n",
    "    '''    \n",
    "    q = q0\n",
    "    lmbda = lmbda0\n",
    "    \n",
    "    # used in calculation of t\n",
    "    nu_m = nu * lmbda.shape[0]\n",
    "    \n",
    "    X = X_tilde[:,:2]\n",
    "    \n",
    "    for i in range(iter):\n",
    "        # step 1: determine t\n",
    "        eh = eta_hat(q=q, h=h, X=X, y=y, lmbda=lmbda)\n",
    "        t = nu_m / eh\n",
    "            \n",
    "        # step 2: compute primal-dual search direction\n",
    "        z = zpd(\n",
    "            Dh=Dh,\n",
    "            grad_f=grad_f,\n",
    "            h=h,\n",
    "            hess_f=hess_f,\n",
    "            lmbda=lmbda,\n",
    "            q=q,\n",
    "            r_cent=r_cent,\n",
    "            r_dual=r_dual,\n",
    "            t=t,\n",
    "            X_tilde=X_tilde,\n",
    "            y=y\n",
    "        )\n",
    "        \n",
    "        # for convenience, split z        \n",
    "        q_pd = z[:q0.shape[0]]\n",
    "        lmbda_pd = z[q0.shape[0]:]\n",
    "\n",
    "        # step 3: backtracking line search\n",
    "        lmbda_neg = lmbda_pd < 0\n",
    "        \n",
    "        # np.amin will fail if no element of lmbda_pd is negative\n",
    "        # (the resulting vector will be of length zero)\n",
    "        if any(lmbda_neg):\n",
    "            c = -lmbda / lmbda_pd\n",
    "            s_max = min(1, np.amin(c[lmbda_neg]))\n",
    "        else:\n",
    "            s_max = 1        \n",
    "\n",
    "        # convenience function to compute the next iterate\n",
    "        xplus = lambda x, x_pd, s: x + s * x_pd\n",
    "        \n",
    "        # initial value of s\n",
    "        s = 0.99 * s_max\n",
    "        \n",
    "        # find s such that h(q^{+}) \\prec 0\n",
    "        qplus = xplus(q, q_pd, s)\n",
    "        \n",
    "        while any(h(v=qplus[:2], b=qplus[2], X=X, y=y) >= 0):\n",
    "            s = s * beta\n",
    "            qplus = xplus(q, q_pd, s)\n",
    "        \n",
    "        # convenience function to construct r_t\n",
    "        def r_t(q, lmbda):\n",
    "            '''\n",
    "            Construct r_t\n",
    "            '''\n",
    "            return np.concatenate(\n",
    "                (\n",
    "                    r_dual(\n",
    "                        q=q, grad_f=grad_f, Dh=Dh, lmbda=lmbda, \n",
    "                        X_tilde=X_tilde, y=y\n",
    "                    ),\n",
    "                    r_cent(q=q, h=h, t=t, lmbda=lmbda, X=X, y=y)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        \n",
    "        # construct r_t and r_{t}^{+}\n",
    "        rt = r_t(q, lmbda)\n",
    "        \n",
    "        lmbdaplus = xplus(lmbda, lmbda_pd, s)\n",
    "        rtplus = r_t(q=qplus, lmbda=lmbdaplus)\n",
    "        \n",
    "        rtnorm = np.linalg.norm(x=rt, ord=2)\n",
    "        \n",
    "        while (\n",
    "            np.linalg.norm(x=rtplus, ord=2) > (1 - alpha * s) * rtnorm\n",
    "        ):\n",
    "            s = s * beta\n",
    "            qplus = xplus(q, q_pd, s)\n",
    "            lmbdaplus = xplus(lmbda, lmbda_pd, s)\n",
    "            rtplus = r_t(q=qplus, lmbda=lmbdaplus)\n",
    "\n",
    "        # finally, update the iterate\n",
    "        q = qplus\n",
    "        lmbda = lmbdaplus\n",
    "        \n",
    "        print('Iteration', i, ': ', q)\n",
    "        \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take 10 steps with the primal-dual algorithm, taking $\\nu=10$ and \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\lambda}_{0}=-\\frac{1}{\\mathbf{h}\\left(\\mathbf{v}_{\\text{int}},b_{\\text{int}}\\right)},\n",
    "$$\n",
    "\n",
    "where $\\left(\\mathbf{v}_{\\text{int}},b_{\\text{int}}\\right)$ is the interior point from part (a).\n",
    "\n",
    "Call py program 1b to calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 :  [0.84608778 9.34182407 0.44208141]\n",
      "Iteration 1 :  [0.48212873 0.59547939 0.09511823]\n",
      "Iteration 2 :  [0.39057708 0.42758012 0.06345998]\n",
      "Iteration 3 :  [0.37040115 0.39336001 0.03548437]\n",
      "Iteration 4 :  [0.36588034 0.37429505 0.00505889]\n",
      "Iteration 5 :  [ 0.37260292  0.36261299 -0.00169696]\n",
      "Iteration 6 :  [ 0.38918246  0.34302408 -0.00622482]\n",
      "Iteration 7 :  [ 0.39225637  0.33877779 -0.00767272]\n",
      "Iteration 8 :  [ 0.39262416  0.33829536 -0.00776826]\n",
      "Iteration 9 :  [ 0.3926536   0.33825047 -0.00777909]\n"
     ]
    }
   ],
   "source": [
    "lmbda0 = -1 / h(v=p0[:2], b=p0[2], X=X, y=y)\n",
    "\n",
    "q_pd = pd(\n",
    "    alpha=0.2,\n",
    "    beta=0.8,\n",
    "    Dh=Dh, \n",
    "    eta_hat=eta_hat, \n",
    "    grad_f=grad_f, \n",
    "    h=h, \n",
    "    hess_f=hess_f, \n",
    "    iter=10, \n",
    "    lmbda0=lmbda0, \n",
    "    nu=10, \n",
    "    q0=p0[:3], \n",
    "    r_cent=r_cent, \n",
    "    r_dual=r_dual, \n",
    "    X_tilde=X_tilde, \n",
    "    y=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The sol for part(c) is Iteration 9 :  [ 0.3926536   0.33825047 -0.00777909]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from parts (b) and (c) are similar, and also observe that the primal-dual method required fewer iterations to converge than the log-barrier method.\n",
    "\n",
    "For $\\mathbf{v},\\mathbf{x}\\in\\mathbb{R}^{2}$, we can plot the affine hyperplane defined by $\\mathbf{v}^{\\mathsf{T}}\\mathbf{x}-b=0$ by noting that\n",
    "\n",
    "$$\n",
    "v_{1}x_{1}+v_{2}x_{2}=0\\implies v_{2}x_{2}=b-v_{1}x_{1}\\implies x_{2}=-\\frac{v_{1}}{v_{2}}x_{1}+\\frac{b}{v_{2}},\n",
    "$$\n",
    "\n",
    "which is a line with slope $-v_{1}/v_{2}$ and intercept $b/v_{2}$. We plot the separating lines from parts (a), (b), and (c) in red, blue, and green, respectively.\n",
    "\n",
    "Recall the theorem that the maximum margin problem is strictly feasible (i.e. Slater's condition holds) if and only if the labelled data is separable. Next we generate SVM to see if our data is linearly separable. If yes, then the duality theory tells us we can instead attempt to solve the dual problem to margin maximization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SVM\n",
    "def abline(slope, intercept, *fmt):\n",
    "    '''\n",
    "    Plot a line from slope and intercept\n",
    "    adapted from https://stackoverflow.com/a/43811762/4557288\n",
    "    :param slope: slope\n",
    "    :param intercept: intercept\n",
    "    :param *fmt: format arguments passed to pyplot.plot\n",
    "    '''\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, *fmt)\n",
    "    \n",
    "    \n",
    "plt.scatter(X[:N,0], X[:N,1])\n",
    "plt.scatter(X[N:,0], X[N:,1])\n",
    "\n",
    "# interior point\n",
    "abline(-p0[0] / p0[1], p0[2] / p0[1], 'r')\n",
    "\n",
    "# log-barrier solution\n",
    "abline(-q_ast[0] / q_ast[1], q_ast[2] / q_ast[1], 'b--')\n",
    "\n",
    "# primal-dual solution\n",
    "abline(-q_pd[0] / q_pd[1], q_pd[2] / q_pd[1], 'g-.')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
